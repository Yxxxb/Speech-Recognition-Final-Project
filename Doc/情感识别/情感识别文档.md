#### 1.阐述一下问题与难点

- 问题

​		本模块要解决的是音频的情感识别问题。具体描述为：首先定义好两个维度的标签，分别是性别（男、女）和情感（angry、happy、sad、calm、fear）。在输入一段音频后，我们需要对这段音频的特征进行分析，预测该音频来源于哪种性别，属于哪一种情感。

- 难点

​		情感本身具有复杂性，使得情感语音数据的采集和整理工作非常困难,进而导致了高质量的情感语料难以获取，对离散情感语音数据库而言,很难获得满足语料自然都和情感纯净度的语音。

​		对维度情感语音数据库的建立而言，困难不在于语料的获取，而在于语料的整理和情感的标注。对于大量的语音数据集进行情感标签的标注十分困难。

​		情感与声学特征之间的关联问题：计算机与人脑的情感识别机制的最初差异就是情感相关声学特征的提取以及情感与声学特征之间的关联方式的确定。因此，如果计算机不能准确地或者以尽可能接近人类的方式对情感语音进行声学特征提取并加以正确的关联和映射，就会使得计算机语音情感识别系统被建立于一个偏离实际的基础之上，从而导致其后的识别机制与人脑处理机制间的差距越来越大,无法达到期望的效果。目前性能较好的特征提取方法是Mel 倒谱系数。同时，如何界定情感声学特征的最优提取时长，或是对不同时长的声学特征进行融合，也需要进行考虑。

#### 2.模型与方法

- 数据集-RAVDESS

​		由于上面所提到“维度情感语音数据库建立”的难点，我们很难亲自进行语料整理和情感标注——因为这种情况下，个人的主观感受可能会造成情感理解的偏差，导致训练出的模型并不能得到准确的结果（令大多数人认可的结果）。因此，我们查阅了许多网上的资料，并最终找到了现在所使用的数据集——RAVDESS英文数据集。

​		RAVDESS包含7356个文件。 该数据库包含24名专业演员(12名女性，12名男性)，用中性的北美口音说出两个词汇匹配的语句。 其中言语包括平静、快乐、悲伤、愤怒、恐惧、惊讶和厌恶的表情， 每个表情都有两种情绪强度(正常和强烈)，外加一种中性表情。在本项目中，我们并没有使用整个数据集，而是提取出平静、快乐、悲伤、愤怒、恐惧五个类别，外加上性别的维度。

​		RAVDESS英文数据集[The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) | Zenodo](https://zenodo.org/record/1188976#.Ycq0B2BBwuU)

- 数据处理方法

  - MFCCS（梅尔倒频谱系数）

  - 提取过程：

    **1）**先对语音进行预加重、分帧和加窗

    **2）**对每一个分析窗，通过STFT得到对应的频谱

    ![](https://www.zhihu.com/equation?tex=X%28n%2C%5Comega%29%3D%5Csum_%7Bm%3D-%5Cinfty%7D%5E%5Cinfty+x%28m%29w%28n-m%29e%5E%7B-j%5Comega+m%7D)

    其中 ![[公式]](https://www.zhihu.com/equation?tex=x%28m%29) 为输入信号， ![[公式]](https://www.zhihu.com/equation?tex=w%28m%29) 是窗函数，它在时间上反转并且有n个样本的偏移量。 ![[公式]](https://www.zhihu.com/equation?tex=X%28n%2C%5Comega%29) 是时间 ![[公式]](https://www.zhihu.com/equation?tex=n) 和频率 ![[公式]](https://www.zhihu.com/equation?tex=%5Comega) 的二维函数，它将信号的时域和频域联系起来，我们可以据此对信号进行时频分析，比如 ![[公式]](https://www.zhihu.com/equation?tex=S%28n%2C%5Comega%29%3D%7CX%28n%2C%5Comega%29%7C%5E2) 就是语音信号所谓的语谱图(Spectrogram)。

    **3）**对上面的频谱通过Mel滤波器组得到Mel频谱

    **4）**在Mel频谱上面进行倒谱分析（取对数，做逆变换，实际逆变换一般是通过DCT离散余弦变换来实现，取DCT后的第2个到第13个系数作为MFCC系数），再加上能量变成13维，获得Mel频率倒谱系数MFCC，这个MFCC就是这帧语音的特征；

    这样，我们对每一段音频进行处理并裁剪，得到了216*13的MFCC特征向量

- 模型

  项目选取了cnn1D一维卷积神经网络进行预测，详细网络架构不再此进行分析。大致过程为：将每一段处理好的音频的特征向量在13的维度上求取均值，得到216*1的向量，并以此为输入，进行模型的训练。每一轮训练结束后执行一次评估，计算模型准确率，以观察模型收敛情况。当训练全部完成后，保存训练好的模型，用于进行大量数据的评估与单条音频的测试。


#### 3.工作流

- 创建数据

  我们利用数据集创建一个Dataframe，其中Dataframe的格式为**<语音文件路径\语音分类标签>**，创建此列表的目的是方便数据预处理时文件的读取。其中，我们的数据集是已经标注好标签的.wav格式的音频文件，语音分类标签是指”angry“，”sad”，“happy”，“calm”以及”fear“等五种情感维度的标签和”male“、”female“两种性别维度的标签。在具体实现过程中，为了简化分类难度与减少训练时间，我们将性别和情感合为一个维度，这样我们就将其转换为了10种类别的多分类问题。

  ```python
  feeling_list=[]
  for item in mylist:
      if item[6:-16]=='02' and int(item[18:-4])%2==0:
          feeling_list.append('female_calm')
      elif item[6:-16]=='02' and int(item[18:-4])%2==1:
          feeling_list.append('male_calm')
      elif item[6:-16]=='03' and int(item[18:-4])%2==0:
          feeling_list.append('female_happy')
      elif item[6:-16]=='03' and int(item[18:-4])%2==1:
          feeling_list.append('male_happy')
      elif item[6:-16]=='04' and int(item[18:-4])%2==0;
  ......
  labels = pd.DataFrame(feeling_list)
  ```
  
  如上图所示，在extract.py中对数据进行标签处理，在创建DataFrame过后，我们对生成的列表文件进行检查，并修改产生错误的表项，最终得到如下包含文件路径与对应标签的列表
  
  ```python
  ,path,label
  0,test_list\female\03-01-02-01-01-01-02.wav,1
  1,test_list\female\03-01-02-01-01-01-04.wav,1
  2,test_list\female\03-01-02-01-01-01-06.wav,1
  3,test_list\female\03-01-02-01-01-01-08.wav,1
  ...
  ```
  
- 数据处理

  在extract.py对数据创建完成后，我们开始对所有训练集中的音频文件进行MFCC特征提取，将得到的特征向量存放的.csv文件，为之后的训练做准备

  ```python
  df = pd.DataFrame(columns=['feature'])
  bookmark=0
  for index,y in enumerate(mylist):
      if mylist[index][6:-16]!='01' and mylist[index][6:-16]!='07' and mylist[index][6:-16]!='08' and mylist[index][:2]!='su' and mylist[index][:1]!='n' and mylist[index][:1]!='d':
          X, sample_rate = librosa.load('RawData/'+y, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)
          sample_rate = np.array(sample_rate)
          mfccs = np.mean(librosa.feature.mfcc(y=X, 
                                              sr=sample_rate, 
                                              n_mfcc=13),
                          axis=0)
          feature = mfccs
  ```

- 模型训练

  运行train.py开始训练模型，使用一维卷积模型，数据输入层设置为[None，216，1]，为了观测模型的收敛程度与预测准确度，在每一个epoch之后我们都对数据重新打乱并进行评估。并在训练完成后将模型保存为.h5文件，用于之后大量测试数据的评估与单条语音的预测。

  ```python
  model = Sequential()
  
  model.add(Conv1D(256, 5,padding='same',
                   input_shape=(216,1)))
  model.add(Activation('relu'))
  model.add(Conv1D(128, 5,padding='same'))
  ......
  model.add(Activation('relu'))
  model.add(Flatten())
  model.add(Dense(10))
  model.add(Activation('softmax'))
  opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)
  model.summary()
  model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])
  ```
  
  我们去掉整个训练部分，避免不必要的长时间训练  
  
  ```python
  cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=700, validation_data=(x_testcnn, y_test))
  ```
  
  训练过程及在模型训练过程中每一轮后得到的损失与准确率如下图所示。可见模型在200轮epoch后收敛速度明显加快。
  
  ![](情感识别.assets/评估.png)
  
  <img src="情感识别.assets/los.png"  />
  
  <img src="情感识别.assets/accuracy.png"  />
  
- 情感预测

  运行预测函数predict.py，先提取出输入音频的MFCC特征并转换为需要的维度，然后提取训练好的模型。利用训练好的模型对输入音频进行预测，得到该音频在每种标签上的概率向量。概率向量中所有分量之和为1，找到该向量中最大的分量所对应的标签，就是我们预测的结果。我们将预测结果通过雷达图表示出来，更为清晰。

  ```python
  def predict(wav_file, loaded_model):
      X, sample_rate = librosa.load(wav_file, res_type='kaiser_fast', duration=2.5, sr=22050 * 2, offset=0.5)
  ......
      livepreds = loaded_model.predict(twodim,
                                       batch_size=32,
                                       verbose=1)
      return livepreds
  ```

  ```python
  def Judge_render(livepreds):
      ifmale = 0
      iffemale = 0
      livepreds = livepreds.reshape(10, )
      for i in range(5):
          iffemale = iffemale + livepreds[i]
      for j in range(5, 10):
          ifmale = ifmale + livepreds[j]
      if ifmale > iffemale:
          return "male", livepreds[5:10]
      else:
          return "female", livepreds[0:5]
  ```

  ![](情感识别.assets/Figure_1.png)

  > [[2.1368088e-01 4.1267309e-02 1.4035265e-07 3.3199336e-02 1.1228149e-01
  > 3.0333758e-05 1.8161860e-01 7.8319320e-03 4.0111807e-01 8.9718904e-03]] 
  > male

#### 4.结果分析

我们对所构建的模型进行结果分析，计算模型的准确率、回归率、f1-score、ROC与AUC等参数，不同类别的模型由于输出形式不同所测试的评估值可能不尽相同。

我们对训练完成后保存的模型进行评估，其中我们选用了与训练集完全不同的测试集。该测试集共包含了959个音频文件，同样我们先使用extract.py提取出<路径/标签>列表，这是音频文件标注好的真实标签；同时，我们将音频文件预测结果（标签）存放到另一个列表。通过对这两个列表进行多个维度的处理比较，计算出上述评估参数。

> Micro precision 0.9103232533889468
> Micro recall 0.9103232533889468
> Micro f1-score 0.9103232533889468
>
> Each class ROC
> 0.9892936461954422
> 0.9964271919660099
> 0.9850086906141368
> 0.9914662997296253
> 0.9901264967168791
> 0.9944838740826574
> 0.9922392787524366
> 0.9900661452298185
> 0.9872175550405563
> 0.9778872151409811
>
> AUC: 0.989140679092956

可见模型在测试集上的准确率与回归率等参数都非常好，训练出的模型是比较优秀的。

#### 5.优缺点

此部分比较优秀的实现了声音情感预测与性别预测，并通过可视化雷达图将预测概率反映出来。同时，利用训练得到的一维卷积神经网络模型在大量测试集上得到良好的准确率、回归率等，说明模型预测成功率较高，可以有效预测单独的音频，这实现了我们的最初目的。同时本项目也具有一定的缺陷。首先是对于输入音频的长度具有限制，由于情感识别要求输入的语音不能过短，所以在模型训练时将特征向量都截取到216维，当输入的音频特征少于216维时，将无法进行预测。此外，在噪声较大、或说话人数过多时，情感预测会出现偏差，因为声音中可能包含多种情感特征与性别特征。