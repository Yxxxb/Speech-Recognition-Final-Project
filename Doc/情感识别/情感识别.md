# 情感识别

思路：利用训练集，提取出不同情感类型音频的特征向量，利用一维卷积进行训练得到模型。通过大量测试集进行模型评估，计算出准确率和回归率。当模型评估较优秀后，使用单独语音进行测试，对结果进行分析。

relu 多分类 softmax 

对求出的mfcc做均值

216×13 变成216*1一维的

=》多分类 普通NN三层 

![image-20211228205739609](G:\LearningSource\speech recognition\Final-Project-Myself\Speech-Recognition-Final-Project\Doc\情感识别\情感识别.assets\image-20211228205739609.png)

### 数据创建

- RAVDESS英文数据集[The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) | Zenodo](https://zenodo.org/record/1188976#.Ycq0B2BBwuU)

- 对数据进行打标签。每段音频被分为两个维度，第一个维度为性别，第二个维度为情感（happy，sad，calm，angry，fear），我们将维度变为一维，得到了10中标签
- 通过librosa库对音频文件进行特征提取，将每个音频文件的mfcc特征转换为216×13维的特征向量，并存放到train.csv文件中，完成数据创建

### 模型训练

- 模型训练使用cnn1D一维卷积模型，通过setup函数创建一维卷积模型，并对卷积模型的维度进行调整。
- 将train.csv读出，带有标签的特征向量输入到神经网络，采用独热编码进行特征处理，使用dnn深度神经网络训练数据，得到训练集上的损失率和准确率。
- 使用验证集进行验证，对每一条输入的语音文件，会预测得到一个10×1维的score向量，向量各个维度之和为1，每个维度的分量表示该语音对应此维度标签的概率。取出拥有最大分量的维度，其对应情感识别的结果。将预测结果与真实标签作比较，计算出正确率。
- 将训练好的模型保存为.h5格式的文件。

### 模型评估

- 使用全新的数据集对模型进行评估

- 使用extract函数创建Dataframe记录音频文件路径及其真实标签

- 加载训练好的.h5模型，将数据集中所有音频进行输入预测，得到包含所有预测结果的向量

- 调用函数，对预测结果向量和真实标签向量进行对比处理，得到准确率和回归率，结果如下

  **Micro precision** 0.9103232533889468
  **Micro recall** 0.9103232533889468
  **Micro f1-score** 0.9103232533889468

  **Each class ROC**
  0.9892936461954422
  0.9964271919660099
  0.9850086906141368
  0.9914662997296253
  0.9901264967168791
  0.9944838740826574
  0.9922392787524366
  0.9900661452298185
  0.9872175550405563
  0.9778872151409811

  **AUC:** 0.989140679092956

- 输入单条语音数据，得到详细预测结果

![Figure_1](D:\github\Speech-Recognition-Final-Project\Doc\情感识别\Figure_1.png)

[[2.1368088e-01 4.1267309e-02 1.4035265e-07 3.3199336e-02 1.1228149e-01
  3.0333758e-05 1.8161860e-01 7.8319320e-03 4.0111807e-01 8.9718904e-03]] 
 male

### 问题分析

​		最开始尝试建立多层感知器和长期短期记忆模型，但效果不好，精确度很低。而后选择从小的开始，而不是为了让它变得复杂而添加太多的层。 在使用多层进行测试后，该模型对测试数据给出了高于70%  的验证精度。





